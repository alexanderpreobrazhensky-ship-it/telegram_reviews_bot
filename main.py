import os
import re
import json
import logging
import sqlite3
from datetime import datetime
from typing import Dict, List, Tuple
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, ContextTypes, MessageHandler, filters

# –ò–º–ø–æ—Ä—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π
try:
    from transformers import pipeline
    from sentence_transformers import SentenceTransformer, util
    NLP_AVAILABLE = True
except ImportError:
    NLP_AVAILABLE = False
    print("‚ö†Ô∏è NLP –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers torch sentence-transformers")

# ================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ==================
TELEGRAM_BOT_TOKEN = "7917601350:AAFG1E7kHKrNzTXIprNADOzLvxpnrUjAcO4"

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –ø–ª–æ—â–∞–¥–æ–∫
def load_rules():
    rules = {
        'yandex': {
            'prohibited': [
                '–Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω–∞—è –±—Ä–∞–Ω—å', '–æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è', '—É–≥—Ä–æ–∑—ã',
                '—Ä–µ–∫–ª–∞–º–∞ —Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö —É—Å–ª—É–≥', '—Ä–∞–∑–≥–ª–∞—à–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö',
                '–∫–ª–µ–≤–µ—Ç–∞', '—Ñ–µ–π–∫–æ–≤—ã–µ –æ—Ç–∑—ã–≤—ã', '—Å–ø–∞–º'
            ],
            'min_words': 10,
            'max_emojis': 3,
            'require_details': True
        },
        '2gis': {
            'prohibited': [
                '–º–∞—Ç–µ—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è', '–ª–∏—á–Ω—ã–µ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è',
                '–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è —Ä–µ–∫–ª–∞–º–∞', '–∑–∞–≤–µ–¥–æ–º–æ –ª–æ–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
                '–º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –æ–¥–Ω–æ—Ç–∏–ø–Ω—ã–µ –æ—Ç–∑—ã–≤—ã'
            ],
            'min_words': 5,
            'require_rating_explanation': True
        }
    }
    return rules

RULES = load_rules()

# ================== –ù–ï–ô–†–û–°–ï–¢–ò ==================
class ReviewAnalyzer:
    def __init__(self):
        self.sentiment_analyzer = None
        self.similarity_model = None
        self.init_models()
    
    def init_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        if not NLP_AVAILABLE:
            return
            
        try:
            # –ú–æ–¥–µ–ª—å –¥–ª—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (—Ä—É—Å—Å–∫–∞—è)
            self.sentiment_analyzer = pipeline(
                "sentiment-analysis",
                model="blanchefort/rubert-base-cased-sentiment"
            )
            
            # –ú–æ–¥–µ–ª—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –Ω–∞—Ä—É—à–µ–Ω–∏–π
            self.similarity_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            print("‚úÖ –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")

    def analyze_sentiment(self, text: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é"""
        if not self.sentiment_analyzer:
            return {'label': 'NEUTRAL', 'score': 0.5}
        
        try:
            result = self.sentiment_analyzer(text[:512])[0]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            return {
                'label': result['label'],  # POSITIVE/NEGATIVE/NEUTRAL
                'score': float(result['score'])
            }
        except:
            return {'label': 'NEUTRAL', 'score': 0.5}

    def check_violations(self, text: str, platform: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –Ω–∞—Ä—É—à–µ–Ω–∏–π –ø—Ä–∞–≤–∏–ª –ø–ª–æ—â–∞–¥–∫–∏"""
        violations = []
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for rule in RULES[platform]['prohibited']:
            if rule in text_lower:
                violations.append({
                    'rule': rule,
                    'type': 'keyword',
                    'confidence': 0.9,
                    'evidence': f"–°–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø—Ä–µ—â–µ–Ω–Ω–æ–µ: '{rule}'"
                })
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–∞—Ä—É—à–µ–Ω–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å –º–æ–¥–µ–ª—å)
        if self.similarity_model:
            try:
                text_embedding = self.similarity_model.encode(text, convert_to_tensor=True)
                
                for rule in RULES[platform]['prohibited']:
                    rule_embedding = self.similarity_model.encode(rule, convert_to_tensor=True)
                    similarity = util.cos_sim(text_embedding, rule_embedding).item()
                    
                    if similarity > 0.7:  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
                        violations.append({
                            'rule': rule,
                            'type': 'semantic',
                            'confidence': similarity,
                            'evidence': f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.2%}"
                        })
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
        if platform == 'yandex':
            words = text.split()
            if len(words) < RULES[platform]['min_words']:
                violations.append({
                    'rule': '–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–∑—ã–≤–∞',
                    'type': 'formal',
                    'confidence': 1.0,
                    'evidence': f"–í—Å–µ–≥–æ {len(words)} —Å–ª–æ–≤, —Ç—Ä–µ–±—É–µ—Ç—Å—è {RULES[platform]['min_words']}"
                })
            
            emoji_count = len(re.findall(r'[\U00010000-\U0010ffff]', text))
            if emoji_count > RULES[platform]['max_emojis']:
                violations.append({
                    'rule': '–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —ç–º–æ–¥–∑–∏',
                    'type': 'formal',
                    'confidence': 1.0,
                    'evidence': f"–ù–∞–π–¥–µ–Ω–æ {emoji_count} —ç–º–æ–¥–∑–∏"
                })
        
        return violations

analyzer = ReviewAnalyzer()

# ================== –ë–ê–ó–ê –î–ê–ù–ù–´–• ==================
def init_db():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    conn = sqlite3.connect('reviews.db')
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS reviews (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            platform TEXT,
            text TEXT,
            rating INTEGER,
            sentiment_label TEXT,
            sentiment_score REAL,
            violations_json TEXT,
            complaint_generated BOOLEAN DEFAULT 0,
            complaint_text TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS complaints (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            review_id INTEGER,
            platform TEXT,
            complaint_text TEXT,
            status TEXT DEFAULT 'draft',
            submitted_at TIMESTAMP,
            FOREIGN KEY (review_id) REFERENCES reviews (id)
        )
    ''')
    
    conn.commit()
    conn.close()

# ================== –¢–ï–ö–°–¢–´ –ñ–ê–õ–û–ë ==================
def generate_complaint(review_text: str, violations: List[Dict], platform: str) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –∂–∞–ª–æ–±—ã"""
    
    templates = {
        'yandex': """
–£–≤–∞–∂–∞–µ–º–∞—è —Å–ª—É–∂–±–∞ –º–æ–¥–µ—Ä–∞—Ü–∏–∏ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç!

–ü—Ä–æ—Å–∏–º —É–¥–∞–ª–∏—Ç—å –æ—Ç–∑—ã–≤ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –Ω–∞—Ä—É—à–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã:

–ù–∞—Ä—É—à–µ–Ω–∏—è:
{violations_list}

–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞:
"{review_text}"

–î–∞–Ω–Ω—ã–π –æ—Ç–∑—ã–≤:
1. –°–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
2. –ù–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
3. –ù–∞—Ä—É—à–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –æ—Ç–∑—ã–≤–æ–≤

–ü—Ä–æ—Å–∏–º —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∂–∞–ª–æ–±—É –∏ —É–¥–∞–ª–∏—Ç—å –æ—Ç–∑—ã–≤ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø—É–Ω–∫—Ç–æ–º {rule_section} –ø—Ä–∞–≤–∏–ª –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç.

–° —É–≤–∞–∂–µ–Ω–∏–µ–º,
–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è —Ç–µ—Ö—Ü–µ–Ω—Ç—Ä–∞ ¬´–õ–∏—Ä–∞¬ª
        """,
        
        '2gis': """
–£–≤–∞–∂–∞–µ–º–∞—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è 2–ì–ò–°!

–ü—Ä–æ—Å–∏–º —É–¥–∞–ª–∏—Ç—å –æ—Ç–∑—ã–≤, –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –Ω–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ, –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –ø—Ä–∏—á–∏–Ω–∞–º:

–í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã:
{violations_list}

–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞:
"{review_text}"

–û—Å–Ω–æ–≤–∞–Ω–∏—è –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:
- –û—Ç–∑—ã–≤ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
- –ù–∞—Ä—É—à–µ–Ω—ã –ø—Ä–∞–≤–∏–ª–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- –ü—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞–∫—Ä—É—Ç–∫–∏/—Ñ–µ–π–∫–∞

–ü—Ä–æ—Å–∏–º –ø—Ä–∏–Ω—è—Ç—å –º–µ—Ä—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ–º 2–ì–ò–°.

–° —É–≤–∞–∂–µ–Ω–∏–µ–º,
–¢–µ—Ö—Ü–µ–Ω—Ç—Ä ¬´–õ–∏—Ä–∞¬ª
        """
    }
    
    violations_list = "\n".join([
        f"- {v['rule']} ({v['type']}, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {v['confidence']:.0%})"
        for v in violations[:5]  # –ë–µ—Ä–µ–º 5 —Å–∞–º—ã—Ö —Å–µ—Ä—å–µ–∑–Ω—ã—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π
    ])
    
    complaint = templates[platform].format(
        violations_list=violations_list,
        review_text=review_text[:500],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        rule_section="5.2" if platform == 'yandex' else "3.1"
    )
    
    return complaint.strip()

# ================== –ö–û–ú–ê–ù–î–´ –ë–û–¢–ê ==================
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO
)
logger = logging.getLogger(__name__)

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /start"""
    await update.message.reply_text(
        "üîß **–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –æ—Ç–∑—ã–≤–æ–≤ —Ç–µ—Ö—Ü–µ–Ω—Ç—Ä–∞ ¬´–õ–∏—Ä–∞¬ª**\n\n"
        "–Ø –ø–æ–º–æ–≥–∞—é –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–∑—ã–≤—ã –Ω–∞ –Ø–Ω–¥–µ–∫—Å –∏ 2–ì–ò–°.\n\n"
        "üìã **–ö–æ–º–∞–Ω–¥—ã:**\n"
        "/analyze_yandex <—Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞> - –∞–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–∞ –¥–ª—è –Ø–Ω–¥–µ–∫—Å\n"
        "/analyze_gis <—Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞> - –∞–Ω–∞–ª–∏–∑ –¥–ª—è 2–ì–ò–°\n"
        "/stats - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ—Ç–∑—ã–≤–∞–º\n"
        "/complaint <id –æ—Ç–∑—ã–≤–∞> - –ø–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã\n\n"
        "–ü—Ä–∏–º–µ—Ä:\n"
        "/analyze_yandex –û—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å! –ú–∞—Å—Ç–µ—Ä–∞ –Ω–µ–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω—ã, –≤—Å—ë —Å–¥–µ–ª–∞–ª–∏ –∫—Ä–∏–≤–æ. –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!"
    )

async def analyze_yandex(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ê–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–∞ –¥–ª—è –Ø–Ω–¥–µ–∫—Å"""
    await analyze_review(update, context, 'yandex')

async def analyze_gis(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ê–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–∞ –¥–ª—è 2–ì–ò–°"""
    await analyze_review(update, context, '2gis')

async def analyze_review(update: Update, context: ContextTypes.DEFAULT_TYPE, platform: str):
    """–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–∞"""
    if not context.args:
        await update.message.reply_text(
            f"‚ùó –£–∫–∞–∂–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞\n"
            f"–ü—Ä–∏–º–µ—Ä: /analyze_{'yandex' if platform == 'yandex' else 'gis'} "
            f"–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ —Å —Ä–µ–π—Ç–∏–Ω–≥–æ–º 2 –∑–≤–µ–∑–¥—ã"
        )
        return
    
    review_text = " ".join(context.args)
    user = update.effective_user
    
    # –ê–Ω–∞–ª–∏–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é
    sentiment = analyzer.analyze_sentiment(review_text)
    violations = analyzer.check_violations(review_text, platform)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∂–∞–ª–æ–±—ã –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞—Ä—É—à–µ–Ω–∏—è
    complaint = None
    if violations:
        complaint = generate_complaint(review_text, violations, platform)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
    conn = sqlite3.connect('reviews.db')
    cursor = conn.cursor()
    cursor.execute('''
        INSERT INTO reviews (platform, text, sentiment_label, sentiment_score, violations_json, complaint_generated, complaint_text)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    ''', (
        platform,
        review_text,
        sentiment['label'],
        sentiment['score'],
        json.dumps(violations, ensure_ascii=False),
        bool(complaint),
        complaint
    ))
    review_id = cursor.lastrowid
    conn.commit()
    conn.close()
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    response = f"üìä **–ê–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–∞ ({platform.upper()})**\n\n"
    response += f"üìù **–¢–µ–∫—Å—Ç:** {review_text[:200]}...\n\n"
    response += f"üéØ **–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** {sentiment['label']} ({sentiment['score']:.0%})\n\n"
    
    if violations:
        response += f"üö® **–ù–∞—Ä—É—à–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω—ã:** {len(violations)}\n"
        for i, v in enumerate(violations[:3], 1):
            response += f"{i}. {v['rule']} ({v['confidence']:.0%})\n"
        
        response += f"\nüìÑ **–ñ–∞–ª–æ–±–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞:** –î–∞ (ID: {review_id})\n"
        response += f"üìã **–î–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞:** /complaint_{review_id}"
    else:
        response += "‚úÖ **–ù–∞—Ä—É—à–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ**\n"
        response += "–û—Ç–∑—ã–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–∞–≤–∏–ª–∞–º –ø–ª–æ—â–∞–¥–∫–∏"
    
    await update.message.reply_text(response)

async def show_complaint(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã"""
    if not context.args:
        await update.message.reply_text("–£–∫–∞–∂–∏—Ç–µ ID –æ—Ç–∑—ã–≤–∞: /complaint 1")
        return
    
    try:
        review_id = int(context.args[0])
        conn = sqlite3.connect('reviews.db')
        cursor = conn.cursor()
        cursor.execute('SELECT platform, complaint_text FROM reviews WHERE id = ?', (review_id,))
        result = cursor.fetchone()
        conn.close()
        
        if not result or not result[1]:
            await update.message.reply_text(f"–ñ–∞–ª–æ–±–∞ –¥–ª—è –æ—Ç–∑—ã–≤–∞ #{review_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
        
        platform, complaint_text = result
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∂–∞–ª–æ–±—ã —á–∞—Å—Ç—è–º–∏ (Telegram –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ 4096 —Å–∏–º–≤–æ–ª–æ–≤)
        chunks = [complaint_text[i:i+4000] for i in range(0, len(complaint_text), 4000)]
        
        await update.message.reply_text(f"üìÑ **–¢–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã –¥–ª—è {platform.upper()}** (ID: {review_id}):\n\n")
        
        for i, chunk in enumerate(chunks, 1):
            await update.message.reply_text(f"–ß–∞—Å—Ç—å {i}:\n```\n{chunk}\n```")
        
        await update.message.reply_text(
            f"‚úÖ **–ñ–∞–ª–æ–±–∞ –≥–æ—Ç–æ–≤–∞ –∫ –æ—Ç–ø—Ä–∞–≤–∫–µ**\n\n"
            f"–ß—Ç–æ –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ:\n"
            f"1. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ç–µ–∫—Å—Ç –≤—ã—à–µ\n"
            f"2. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ç–∑—ã–≤–∞\n"
            f"3. –ù–∞–∂–º–∏—Ç–µ '–ü–æ–∂–∞–ª–æ–≤–∞—Ç—å—Å—è'\n"
            f"4. –í—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã\n\n"
            f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: /stats"
        )
        
    except ValueError:
        await update.message.reply_text("ID –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º")
    except Exception as e:
        await update.message.reply_text(f"–û—à–∏–±–∫–∞: {e}")

async def stats(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ—Ç–∑—ã–≤–∞–º"""
    conn = sqlite3.connect('reviews.db')
    cursor = conn.cursor()
    
    cursor.execute('SELECT COUNT(*) FROM reviews')
    total = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM reviews WHERE complaint_generated = 1')
    with_complaints = cursor.fetchone()[0]
    
    cursor.execute('''
        SELECT platform, COUNT(*) 
        FROM reviews 
        WHERE complaint_generated = 1 
        GROUP BY platform
    ''')
    by_platform = cursor.fetchall()
    
    conn.close()
    
    response = "üìà **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–∑—ã–≤–æ–≤**\n\n"
    response += f"üìä –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {total} –æ—Ç–∑—ã–≤–æ–≤\n"
    response += f"üö® –° –Ω–∞—Ä—É—à–µ–Ω–∏—è–º–∏: {with_complaints} ({with_complaints/max(total,1)*100:.0f}%)\n\n"
    
    if by_platform:
        response += "**–ü–æ –ø–ª–æ—â–∞–¥–∫–∞–º:**\n"
        for platform, count in by_platform:
            response += f"- {platform.upper()}: {count} –∂–∞–ª–æ–±\n"
    
    response += f"\nüîç **–ü–æ—Å–ª–µ–¥–Ω–∏–µ –æ—Ç–∑—ã–≤—ã:** /recent_5"
    
    await update.message.reply_text(response)

async def recent_reviews(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ—Ç–∑—ã–≤—ã"""
    limit = 5
    conn = sqlite3.connect('reviews.db')
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT id, platform, text, created_at 
        FROM reviews 
        ORDER BY id DESC 
        LIMIT ?
    ''', (limit,))
    
    reviews = cursor.fetchall()
    conn.close()
    
    if not reviews:
        await update.message.reply_text("–ï—â–µ –Ω–µ—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤")
        return
    
    response = f"üìã **–ü–æ—Å–ª–µ–¥–Ω–∏–µ {limit} –æ—Ç–∑—ã–≤–æ–≤**\n\n"
    
    for idx, (review_id, platform, text, created_at) in enumerate(reviews, 1):
        preview = text[:100] + "..." if len(text) > 100 else text
        response += f"{idx}. **{platform.upper()}** (ID: {review_id})\n"
        response += f"   {preview}\n"
        response += f"   üìÖ {created_at[:10]}\n"
        response += f"   üìÑ /complaint_{review_id}\n\n"
    
    await update.message.reply_text(response)

# ================== –ó–ê–ü–£–°–ö ==================
def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
    init_db()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π
    if not NLP_AVAILABLE:
        print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers torch sentence-transformers")
        print("–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
    
    # –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
    try:
        print("ü§ñ –ó–∞–ø—É—Å–∫–∞—é –±–æ—Ç–∞-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –æ—Ç–∑—ã–≤–æ–≤...")
        app = ApplicationBuilder().token(TELEGRAM_BOT_TOKEN).build()
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–æ–º–∞–Ω–¥
        app.add_handler(CommandHandler("start", start))
        app.add_handler(CommandHandler("analyze_yandex", analyze_yandex))
        app.add_handler(CommandHandler("analyze_gis", analyze_gis))
        
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –∂–∞–ª–æ–±
        app.add_handler(CommandHandler("complaint", show_complaint))
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        app.add_handler(CommandHandler("stats", stats))
        app.add_handler(CommandHandler("recent_5", recent_reviews))
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ã—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, 
                                      lambda u, c: u.message.reply_text("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—ã –∏–∑ /start")))
        
        logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω")
        print("‚úÖ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –æ—Ç–∑—ã–≤–æ–≤ –∑–∞–ø—É—â–µ–Ω!")
        print("üîó –ò—â–∏—Ç–µ –≤ Telegram: /start")
        
        app.run_polling(drop_pending_updates=True)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: {e}")
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        raise

if __name__ == "__main__":
    main()
