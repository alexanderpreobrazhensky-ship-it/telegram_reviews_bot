import os
import json
import logging
import threading
import re
from datetime import datetime, timedelta
from typing import Any, Dict, Optional, List, Tuple

import requests
from flask import Flask, request

# -------------------------
# Logging
# -------------------------
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("telegram-reviews-bot")

# -------------------------
# Telegram env
# -------------------------
BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
if not BOT_TOKEN:
    raise ValueError("Missing TELEGRAM_BOT_TOKEN")

WEBHOOK_URL = os.getenv("WEBHOOK_URL")
if not WEBHOOK_URL:
    raise ValueError("Missing WEBHOOK_URL")

BOT_PATH_SECRET = os.getenv("BOT_PATH_SECRET", "hook")
WEBHOOK_PATH = f"/webhook/{BOT_PATH_SECRET}"

TG_TIMEOUT = float(os.getenv("TG_TIMEOUT", "10"))
AI_TIMEOUT = float(os.getenv("AI_TIMEOUT", "15"))

# -------------------------
# Admins
# -------------------------
REPORT_CHAT_IDS_RAW = (os.getenv("REPORT_CHAT_IDS") or "").strip()

def parse_admin_ids(raw: str) -> List[int]:
    if not raw:
        return []
    out: List[int] = []
    for part in raw.split(","):
        part = part.strip()
        if not part:
            continue
        try:
            out.append(int(part))
        except Exception:
            logger.warning("Invalid REPORT_CHAT_IDS entry: %r", part)
    return out

ADMIN_CHAT_IDS = parse_admin_ids(REPORT_CHAT_IDS_RAW)
if not ADMIN_CHAT_IDS:
    logger.warning("REPORT_CHAT_IDS is empty -> admin commands allowed for everyone (NOT recommended).")

def is_admin(chat_id: int) -> bool:
    return (not ADMIN_CHAT_IDS) or (chat_id in ADMIN_CHAT_IDS)

# -------------------------
# Business context (optional)
# -------------------------
BUSINESS_CONTEXT = (os.getenv("BUSINESS_CONTEXT") or "").strip() or None
BRANCH_CITY = (os.getenv("BRANCH_CITY") or "").strip() or None

# -------------------------
# AI multi-engine env
# -------------------------
AI_ENGINE = (os.getenv("AI_ENGINE") or "deepseek").strip().lower()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
GROK_API_KEY = os.getenv("GROK_API_KEY") or os.getenv("XAI_API_KEY")

GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.0-flash")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
DEEPSEEK_MODEL = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
GROK_MODEL = os.getenv("GROK_MODEL", "grok-beta")

GEMINI_URL = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent"
OPENAI_URL = os.getenv("OPENAI_URL", "https://api.openai.com/v1/chat/completions")

DEEPSEEK_BASE_URL = (os.getenv("DEEPSEEK_BASE_URL") or "").strip()
if DEEPSEEK_BASE_URL:
    DEEPSEEK_URL = DEEPSEEK_BASE_URL.rstrip("/") + "/chat/completions"
else:
    DEEPSEEK_URL = os.getenv("DEEPSEEK_URL", "https://api.deepseek.com/chat/completions")

GROK_URL = os.getenv("GROK_URL", "https://api.x.ai/v1/chat/completions")

# -------------------------
# Cron (weekly report)
# -------------------------
CRON_TOKEN = (os.getenv("CRON_TOKEN") or "").strip()  # required for /cron endpoint security

# -------------------------
# DB (Postgres on Railway or SQLite fallback)
# -------------------------
DATABASE_URL = (os.getenv("DATABASE_URL") or "").strip()
USE_POSTGRES = DATABASE_URL.startswith("postgres://") or DATABASE_URL.startswith("postgresql://")
SQL_PARAM = "%s" if USE_POSTGRES else "?"

if USE_POSTGRES:
    try:
        import psycopg
        from psycopg.types.json import Json
    except Exception as e:
        raise RuntimeError(
            "Postgres detected (DATABASE_URL set) but psycopg is not available. "
            "Install psycopg[binary]==3.x in requirements.txt"
        ) from e

def db_connect():
    if USE_POSTGRES:
        return psycopg.connect(DATABASE_URL)
    else:
        import sqlite3
        conn = sqlite3.connect("reviews.db")
        conn.row_factory = sqlite3.Row
        return conn

def db_init():
    # reviews table (as before)
    if USE_POSTGRES:
        ddl_reviews = """
        CREATE TABLE IF NOT EXISTS reviews (
          id               SERIAL PRIMARY KEY,
          source           TEXT NOT NULL,
          rating           INTEGER NULL,
          author           TEXT NULL,
          url              TEXT NULL,
          published_at     TIMESTAMP NULL,
          text             TEXT NOT NULL,
          added_by_user_id BIGINT NULL,
          added_by_chat_id BIGINT NULL,
          created_at       TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
        );
        """
        ddl_analyses = """
        CREATE TABLE IF NOT EXISTS review_analyses (
          id           SERIAL PRIMARY KEY,
          review_id    BIGINT NULL,
          ai_engine    TEXT NOT NULL,
          input_json   JSONB NOT NULL,
          result_json  JSONB NOT NULL,
          created_at   TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
        );
        CREATE INDEX IF NOT EXISTS idx_review_analyses_created_at ON review_analyses(created_at);
        CREATE INDEX IF NOT EXISTS idx_review_analyses_review_id ON review_analyses(review_id);
        """
    else:
        ddl_reviews = """
        CREATE TABLE IF NOT EXISTS reviews (
          id               INTEGER PRIMARY KEY AUTOINCREMENT,
          source           TEXT NOT NULL,
          rating           INTEGER NULL,
          author           TEXT NULL,
          url              TEXT NULL,
          published_at     TEXT NULL,
          text             TEXT NOT NULL,
          added_by_user_id INTEGER NULL,
          added_by_chat_id INTEGER NULL,
          created_at       TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
        );
        """
        ddl_analyses = """
        CREATE TABLE IF NOT EXISTS review_analyses (
          id           INTEGER PRIMARY KEY AUTOINCREMENT,
          review_id    INTEGER NULL,
          ai_engine    TEXT NOT NULL,
          input_json   TEXT NOT NULL,
          result_json  TEXT NOT NULL,
          created_at   TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
        );
        """

    conn = db_connect()
    try:
        cur = conn.cursor()
        cur.execute(ddl_reviews)
        if USE_POSTGRES:
            # ddl_analyses contains multiple statements
            for stmt in [s.strip() for s in ddl_analyses.split(";") if s.strip()]:
                cur.execute(stmt + ";")
        else:
            cur.execute(ddl_analyses)
        conn.commit()
        logger.info("DB init OK (postgres=%s)", USE_POSTGRES)
    finally:
        conn.close()

db_init()

# -------------------------
# Flask
# -------------------------
app = Flask(__name__)

# -------------------------
# Helpers: redact secrets in logs
# -------------------------
def _redact(s: str) -> str:
    if not s:
        return s
    for key in [GEMINI_API_KEY, OPENAI_API_KEY, DEEPSEEK_API_KEY, GROK_API_KEY, BOT_TOKEN, CRON_TOKEN]:
        if key and key in s:
            s = s.replace(key, "***REDACTED***")
    return s

# -------------------------
# Telegram API helpers
# -------------------------
def tg_api(method: str, payload: Dict[str, Any]) -> Optional[requests.Response]:
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/{method}"
    try:
        return requests.post(url, json=payload, timeout=TG_TIMEOUT)
    except Exception as e:
        logger.exception("Telegram API exception %s: %s", method, e)
        return None

def tg_send_message(chat_id: int, text: str, reply_to: Optional[int] = None,
                    reply_markup: Optional[Dict[str, Any]] = None) -> None:
    payload: Dict[str, Any] = {"chat_id": chat_id, "text": text}
    if reply_to is not None:
        payload["reply_to_message_id"] = reply_to
    if reply_markup is not None:
        payload["reply_markup"] = reply_markup

    resp = tg_api("sendMessage", payload)
    if resp is not None and resp.status_code != 200:
        logger.error("sendMessage failed: %s", _redact(resp.text[:800]))

def tg_answer_callback_query(callback_query_id: str, text: Optional[str] = None, show_alert: bool = False) -> None:
    payload: Dict[str, Any] = {"callback_query_id": callback_query_id}
    if text:
        payload["text"] = text
    payload["show_alert"] = bool(show_alert)

    resp = tg_api("answerCallbackQuery", payload)
    if resp is not None and resp.status_code != 200:
        logger.error("answerCallbackQuery failed: %s", _redact(resp.text[:400]))

def set_webhook() -> None:
    full_url = WEBHOOK_URL.rstrip("/") + WEBHOOK_PATH
    logger.info("Setting webhook: %s", full_url)
    try:
        r = requests.post(
            f"https://api.telegram.org/bot{BOT_TOKEN}/setWebhook",
            json={"url": full_url},
            timeout=TG_TIMEOUT,
        )
        if r.status_code == 429:
            logger.warning("setWebhook got 429 (ignored): %s", _redact(r.text[:400]))
            return
        if r.status_code != 200:
            logger.error("setWebhook failed status=%s body=%s", r.status_code, _redact(r.text[:800]))
        else:
            logger.info("setWebhook OK: %s", _redact(r.text[:400]))
    except Exception as e:
        logger.exception("set_webhook exception: %s", e)

if os.getenv("DISABLE_WEBHOOK_SETUP", "0") != "1":
    set_webhook()

# -------------------------
# DB operations: reviews (existing)
# -------------------------
def parse_kv_args(arg_str: str) -> Tuple[Dict[str, str], str]:
    tokens = arg_str.strip().split()
    kv: Dict[str, str] = {}
    rest_tokens: List[str] = []
    for t in tokens:
        if "=" in t and not t.lower().startswith("http"):
            k, v = t.split("=", 1)
            k = k.strip().lower()
            v = v.strip().strip('"').strip("'")
            if k and v:
                kv[k] = v
                continue
        rest_tokens.append(t)
    return kv, " ".join(rest_tokens).strip()

def parse_date(s: str) -> Optional[datetime]:
    s = (s or "").strip()
    if not s:
        return None
    try:
        return datetime.fromisoformat(s)
    except Exception:
        return None

def db_insert_review(source: str, text: str, rating: Optional[int], author: Optional[str],
                     url: Optional[str], published_at: Optional[datetime],
                     added_by_user_id: Optional[int], added_by_chat_id: Optional[int]) -> int:
    conn = db_connect()
    try:
        cur = conn.cursor()
        if USE_POSTGRES:
            cur.execute(
                f"""
                INSERT INTO reviews (source, rating, author, url, published_at, text, added_by_user_id, added_by_chat_id)
                VALUES ({SQL_PARAM},{SQL_PARAM},{SQL_PARAM},{SQL_PARAM},{SQL_PARAM},{SQL_PARAM},{SQL_PARAM},{SQL_PARAM})
                RETURNING id
                """,
                (source, rating, author, url, published_at, text, added_by_user_id, added_by_chat_id),
            )
            new_id = cur.fetchone()[0]
        else:
            cur.execute(
                f"""
                INSERT INTO reviews (source, rating, author, url, published_at, text, added_by_user_id, added_by_chat_id)
                VALUES ({SQL_PARAM},{SQL_PARAM},{SQL_PARAM},{SQL_PARAM},{SQL_PARAM},{SQL_PARAM},{SQL_PARAM},{SQL_PARAM})
                """,
                (source, rating, author, url,
                 published_at.isoformat() if published_at else None,
                 text, added_by_user_id, added_by_chat_id),
            )
            new_id = cur.lastrowid
        conn.commit()
        return int(new_id)
    finally:
        conn.close()

def db_get_review(review_id: int) -> Optional[Dict[str, Any]]:
    conn = db_connect()
    try:
        cur = conn.cursor()
        cur.execute(f"SELECT * FROM reviews WHERE id={SQL_PARAM}", (review_id,))
        row = cur.fetchone()
        if not row:
            return None
        if USE_POSTGRES:
            cols = [d[0] for d in cur.description]
            return {cols[i]: row[i] for i in range(len(cols))}
        return dict(row)
    finally:
        conn.close()

def db_list_reviews(limit: int = 10, source: Optional[str] = None) -> List[Dict[str, Any]]:
    conn = db_connect()
    try:
        cur = conn.cursor()
        if source:
            cur.execute(
                f"SELECT * FROM reviews WHERE source={SQL_PARAM} ORDER BY id DESC LIMIT {int(limit)}",
                (source,),
            )
        else:
            cur.execute(f"SELECT * FROM reviews ORDER BY id DESC LIMIT {int(limit)}")
        rows = cur.fetchall()
        if USE_POSTGRES:
            cols = [d[0] for d in cur.description]
            return [{cols[i]: r[i] for i in range(len(cols))} for r in rows]
        return [dict(r) for r in rows]
    finally:
        conn.close()

def db_delete_review(review_id: int) -> bool:
    conn = db_connect()
    try:
        cur = conn.cursor()
        cur.execute(f"DELETE FROM reviews WHERE id={SQL_PARAM}", (review_id,))
        conn.commit()
        return cur.rowcount > 0
    finally:
        conn.close()

def review_preview(r: Dict[str, Any], max_len: int = 220) -> str:
    text = (r.get("text") or "").strip().replace("\n", " ")
    if len(text) > max_len:
        text = text[: max_len - 1] + "‚Ä¶"
    parts = [
        f"#{r.get('id')} [{r.get('source')}]",
        f"rating={r.get('rating')}" if r.get("rating") is not None else None,
        f"url={r.get('url')}" if r.get("url") else None,
    ]
    head = " ".join([p for p in parts if p])
    return f"{head}\n{text}"

# -------------------------
# DB operations: analyses
# -------------------------
def db_insert_analysis(review_id: Optional[int], ai_engine: str,
                       input_obj: Dict[str, Any], result_obj: Dict[str, Any]) -> int:
    conn = db_connect()
    try:
        cur = conn.cursor()
        if USE_POSTGRES:
            cur.execute(
                f"""
                INSERT INTO review_analyses (review_id, ai_engine, input_json, result_json)
                VALUES ({SQL_PARAM},{SQL_PARAM},{SQL_PARAM},{SQL_PARAM})
                RETURNING id
                """,
                (review_id, ai_engine, Json(input_obj), Json(result_obj)),
            )
            new_id = cur.fetchone()[0]
        else:
            cur.execute(
                f"""
                INSERT INTO review_analyses (review_id, ai_engine, input_json, result_json)
                VALUES ({SQL_PARAM},{SQL_PARAM},{SQL_PARAM},{SQL_PARAM})
                """,
                (review_id, ai_engine, json.dumps(input_obj, ensure_ascii=False),
                 json.dumps(result_obj, ensure_ascii=False)),
            )
            new_id = cur.lastrowid
        conn.commit()
        return int(new_id)
    finally:
        conn.close()

def db_get_analysis(analysis_id: int) -> Optional[Dict[str, Any]]:
    conn = db_connect()
    try:
        cur = conn.cursor()
        cur.execute(f"SELECT * FROM review_analyses WHERE id={SQL_PARAM}", (analysis_id,))
        row = cur.fetchone()
        if not row:
            return None
        if USE_POSTGRES:
            cols = [d[0] for d in cur.description]
            obj = {cols[i]: row[i] for i in range(len(cols))}
            return obj
        obj = dict(row)
        # sqlite: parse json fields
        try:
            obj["input_json"] = json.loads(obj.get("input_json") or "{}")
        except Exception:
            obj["input_json"] = {}
        try:
            obj["result_json"] = json.loads(obj.get("result_json") or "{}")
        except Exception:
            obj["result_json"] = {}
        return obj
    finally:
        conn.close()

def db_list_analyses_since(dt: datetime) -> List[Dict[str, Any]]:
    conn = db_connect()
    try:
        cur = conn.cursor()
        if USE_POSTGRES:
            cur.execute(
                f"SELECT * FROM review_analyses WHERE created_at >= {SQL_PARAM} ORDER BY id DESC",
                (dt,),
            )
            rows = cur.fetchall()
            cols = [d[0] for d in cur.description]
            return [{cols[i]: r[i] for i in range(len(cols))} for r in rows]
        else:
            cur.execute(
                f"SELECT * FROM review_analyses WHERE created_at >= {SQL_PARAM} ORDER BY id DESC",
                (dt.isoformat(),),
            )
            rows = cur.fetchall()
            out = []
            for r in rows:
                d = dict(r)
                try:
                    d["result_json"] = json.loads(d.get("result_json") or "{}")
                except Exception:
                    d["result_json"] = {}
                out.append(d)
            return out
    finally:
        conn.close()

# -------------------------
# CX System prompt (your contract) - STRICT JSON output
# -------------------------
SYSTEM_PROMPT_CX = r"""
–¢–´ ‚Äî –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –º–æ–¥—É–ª—å –¥–ª—è Telegram-–±–æ—Ç–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ —Å–µ—Ä–≤–∏—Å–∞ (CX/Service Quality). –ë–æ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä–µ Telegram –∏ –ø–æ–ª—É—á–∞–µ—Ç –æ—Ç–∑—ã–≤—ã –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –ø–ª–æ—â–∞–¥–æ–∫ (—Å–µ–π—á–∞—Å: 2–ì–ò–° –∏ –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç—ã), —Å –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–æ–π –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –ú–æ–¥–µ–ª—å –ò–ò –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî DeepSeek, –Ω–æ –ª–æ–≥–∏–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.

–¢–í–û–Ø –ó–ê–î–ê–ß–ê (—Å—Ç—Ä–æ–≥–æ):
1) –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–ª–æ—â–∞–¥–∫—É (2–ì–ò–° / –Ø–Ω–¥–µ–∫—Å) –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞.
2) –î–∞—Ç—å –ì–õ–£–ë–û–ö–ò–ô –∞–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–∞: –ø—Ä–∏—á–∏–Ω—ã, —Å–±–æ–∏ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å–æ–≤, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–ª–µ–º—ã (–Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ —Å–µ—Ä–≤–∏—Å–∞), —Ä–∏—Å–∫–∏, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏.
3) –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Ç–∑—ã–≤ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–∞–≤–∏–ª–∞–º –ø–ª–æ—â–∞–¥–∫–∏ (–ø–æ —á–µ–∫-–ª–∏—Å—Ç—É –Ω–∏–∂–µ).
4) –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—É–±–ª–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Ç–∑—ã–≤ (—Ä–∞–∑–Ω—ã–π —Å—Ç–∏–ª—å –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–≥–æ/–Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ/—Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ).
5) –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –ø–ª–æ—â–∞–¥–∫–∏ –ò–õ–ò —Ä–µ–π—Ç–∏–Ω–≥ < 2 (—Ç.–µ. 1 –∑–≤–µ–∑–¥–∞) ‚Äî –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∂–∞–ª–æ–±—É –Ω–∞ –æ—Ç–∑—ã–≤:
   - –î–ª—è 2–ì–ò–°: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã —Å—Ç—Ä–æ–≥–æ ‚â§ 450 —Å–∏–º–≤–æ–ª–æ–≤ (–≤–∫–ª—é—á–∞—è –ø—Ä–æ–±–µ–ª—ã).
   - –î–ª—è –Ø–Ω–¥–µ–∫—Å–∞: –∂–∞–ª–æ–±–∞ –∫—Ä–∞—Ç–∫–∞—è, –ø–æ –¥–µ–ª—É (–±–µ–∑ –ª–∏–º–∏—Ç–∞, –Ω–æ –Ω–µ ‚Äú–ø—Ä–æ—Å—Ç—ã–Ω—è‚Äù).

–í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï (–∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –ø–µ—Ä–µ–¥–∞–Ω–æ; –ù–ï –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã):
- platform: "2gis" | "yandex" | "unknown" (–º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å)
- rating: 1..5 (–º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å)
- review_text: —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
- review_date: –¥–∞—Ç–∞ –æ—Ç–∑—ã–≤–∞ (–º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å)
- business_context: –æ–ø–∏—Å–∞–Ω–∏–µ –±–∏–∑–Ω–µ—Å–∞/—É—Å–ª—É–≥/—Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞ (–º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å)
- branch/city: —Ñ–∏–ª–∏–∞–ª/–≥–æ—Ä–æ–¥ (–º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å)
- meta: –ª—é–±—ã–µ –¥–æ–ø. –ø–æ–ª—è (—è–∑—ã–∫, –∏–º—è –∞–≤—Ç–æ—Ä–∞, —Å—Å—ã–ª–∫–∞, —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –æ—Ç–≤–µ—Ç –∏ —Ç.–ø.)

–û–ë–©–ò–ï –ü–†–ò–ù–¶–ò–ü–´ –ö–ê–ß–ï–°–¢–í–ê:
- –ù–∏–∫–∞–∫–∏—Ö –≤—ã–¥—É–º–∞–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π (–∑–∞–∫–∞–∑—ã, –¥–∞—Ç—ã, —Å—É–º–º—ã, –∏–º–µ–Ω–∞ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤), –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤–æ –≤—Ö–æ–¥–µ.
- –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ ‚Äî —Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –≥–∏–ø–æ—Ç–µ–∑—ã + —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏.
- –¶–∏—Ç–∞—Ç—ã –∏–∑ –æ—Ç–∑—ã–≤–∞ –¥–µ–ª–∞–π –∫–æ—Ä–æ—Ç–∫–∏–º–∏: –¥–æ 12 —Å–ª–æ–≤.
- –ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –≤–µ–∂–ª–∏–≤–æ, –±–µ–∑ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏.
- –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –ø—É–±–ª–∏–∫—É–π –∏ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
- –ù–µ —É–ø–æ–º–∏–Ω–∞–π –ø—É–±–ª–∏—á–Ω–æ ‚Äú–º—ã –ø–æ–¥–∞–¥–∏–º –∂–∞–ª–æ–±—É‚Äù –∏ –Ω–µ —É–≥—Ä–æ–∂–∞–π –∞–≤—Ç–æ—Ä—É.

–®–ê–ì 1. –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–õ–û–©–ê–î–ö–ò (–µ—Å–ª–∏ platform –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç/unknown)
–í–µ—Ä–Ω–∏ platform_detected.value: "2gis" | "yandex" | "unknown", confidence 0..1, signals 2‚Äì5.

–®–ê–ì 2. –ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó –û–¢–ó–´–í–ê
–°—Ñ–æ—Ä–º–∏—Ä—É–π –±–ª–æ–∫–∏: review_summary, sentiment, emotions, aspects, facts_vs_opinions, pain_points,
root_cause_hypotheses, business_process_flags, risks, recommendations, clarifying_questions.

–®–ê–ì 3. –ü–†–û–í–ï–†–ö–ê –ü–û –ü–†–ê–í–ò–õ–ê–ú –ü–õ–û–©–ê–î–ö–ò (policy_check)
–í–µ—Ä–Ω–∏ has_possible_violations, possible_violations (confidence+evidence), notes.

–®–ê–ì 4. –ü–£–ë–õ–ò–ß–ù–´–ô –û–¢–í–ï–¢ –ù–ê –û–¢–ó–´–í (public_reply)
2‚Äì8 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —á–µ–ª–æ–≤–µ—á–Ω–æ, –±–µ–∑ –∫–∞–Ω—Ü–µ–ª—è—Ä–∏—Ç–∞, –±–µ–∑ –ü–î–Ω, –±–µ–∑ —É–≥—Ä–æ–∑, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ.

–®–ê–ì 5. –ñ–ê–õ–û–ë–ê –ù–ê –û–¢–ó–´–í (complaint)
complaint.needed=true –µ—Å–ª–∏:
a) rating < 2 (–µ—Å–ª–∏ rating –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ‚Äî –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å)
–ò–õ–ò b) policy_check.has_possible_violations=true –∏ –∫–ª—é—á–µ–≤–∞—è –ø—Ä–∏—á–∏–Ω–∞ confidence ‚â•0.6
–ò–õ–ò c) —Å–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª ‚Äú–Ω–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤–∏–∑–∏—Ç–∞‚Äù (–∫–∞–∫ –≥–∏–ø–æ—Ç–µ–∑–∞)

–î–ª—è 2–ì–ò–°: complaint.text ‚â§ 450 —Å–∏–º–≤–æ–ª–æ–≤; –≤–µ—Ä–Ω–∏ complaint.char_count.

–í–´–•–û–î–ù–û–ô –§–û–†–ú–ê–¢ (–°–¢–†–û–ì–û: –≤–µ—Ä–Ω—É—Ç—å –¢–û–õ–¨–ö–û JSON, –±–µ–∑ markdown, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π –≤–æ–∫—Ä—É–≥)
{
  "platform_detected": {"value":"2gis|yandex|unknown","confidence":0.0,"signals":["..."]},
  "review_summary":"...",
  "sentiment":{"label":"negative|mixed|neutral|positive","score":0},
  "emotions":[{"name":"...","intensity":0}],
  "aspects":[{"name":"...","weight":0,"evidence":["..."]}],
  "facts_vs_opinions":{"facts":["..."],"opinions":["..."]},
  "pain_points":[{"item":"...","severity":"low|medium|high","evidence":["..."]}],
  "root_cause_hypotheses":[{"hypothesis":"...","confidence":0.0,"evidence":["..."],"process_stage":"..."}],
  "business_process_flags":[{"stage":"...","issue":"...","why_it_matters":"..."}],
  "risks":[{"type":"reputation|ops|finance","level":"low|medium|high","why":"..."}],
  "recommendations":[{"priority":"P0|P1|P2","action":"...","expected_effect":"...","effort":"S|M|L","metric":"..."}],
  "clarifying_questions":["..."],
  "policy_check":{
    "has_possible_violations":true,
    "possible_violations":[{"category":"...","confidence":0.0,"evidence":["..."]}],
    "notes":"..."
  },
  "public_reply":{"tone":"...","text":"..."},
  "complaint":{"needed":false,"reasons":["..."],"text":"...","char_count":0}
}

–î–û–ü. –¢–ï–•–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- JSON –≤–∞–ª–∏–¥–Ω—ã–π: –¥–≤–æ–π–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏, –±–µ–∑ trailing commas.
- –ï—Å–ª–∏ –±–ª–æ–∫ –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º ‚Äî –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç—ã–µ –º–∞—Å—Å–∏–≤—ã/false, –Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è.
""".strip()

# -------------------------
# AI transport (provider-agnostic)
# -------------------------
def call_deepseek(messages: List[Dict[str, str]]) -> str:
    if not DEEPSEEK_API_KEY:
        return "‚ùå DEEPSEEK_API_KEY –Ω–µ –∑–∞–¥–∞–Ω."
    payload = {
        "model": DEEPSEEK_MODEL,
        "messages": messages,
        "temperature": 0.2,
        "stream": False,
    }
    headers = {"Authorization": f"Bearer {DEEPSEEK_API_KEY}", "Content-Type": "application/json"}
    resp = requests.post(DEEPSEEK_URL, json=payload, headers=headers, timeout=AI_TIMEOUT)
    logger.info("DeepSeek status=%s body=%s", resp.status_code, _redact(resp.text[:900]))
    resp.raise_for_status()
    data = resp.json()
    choices = data.get("choices", [])
    if not choices:
        return ""
    msg = choices[0].get("message", {}) or {}
    return (msg.get("content") or "").strip()

def call_openai(messages: List[Dict[str, str]]) -> str:
    if not OPENAI_API_KEY:
        return "‚ùå OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω."
    payload = {
        "model": OPENAI_MODEL,
        "messages": messages,
        "temperature": 0.2,
    }
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    resp = requests.post(OPENAI_URL, json=payload, headers=headers, timeout=AI_TIMEOUT)
    logger.info("OpenAI status=%s body=%s", resp.status_code, _redact(resp.text[:900]))
    resp.raise_for_status()
    data = resp.json()
    choices = data.get("choices", [])
    if not choices:
        return ""
    return ((choices[0].get("message", {}) or {}).get("content") or "").strip()

def call_gemini(messages: List[Dict[str, str]]) -> str:
    # Gemini expects a single user prompt; we concatenate system+user
    if not GEMINI_API_KEY:
        return "‚ùå GEMINI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω."
    combined = []
    for m in messages:
        role = m.get("role")
        content = m.get("content", "")
        if role == "system":
            combined.append("–ò–ù–°–¢–†–£–ö–¶–ò–ò:\n" + content)
        elif role == "user":
            combined.append("–í–•–û–î:\n" + content)
        else:
            combined.append(content)
    prompt = "\n\n".join(combined).strip()

    payload = {
        "contents": [{"role": "user", "parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0.2, "maxOutputTokens": 900},
    }
    resp = requests.post(
        GEMINI_URL,
        params={"key": GEMINI_API_KEY},
        json=payload,
        timeout=AI_TIMEOUT,
    )
    logger.info("Gemini status=%s body=%s", resp.status_code, _redact(resp.text[:900]))
    resp.raise_for_status()
    data = resp.json()
    candidates = data.get("candidates", [])
    if not candidates:
        return ""
    parts = (candidates[0].get("content", {}) or {}).get("parts", []) or []
    return ((parts[0].get("text") if parts else "") or "").strip()

def call_grok(messages: List[Dict[str, str]]) -> str:
    if not GROK_API_KEY:
        return "‚ùå GROK_API_KEY (–∏–ª–∏ XAI_API_KEY) –Ω–µ –∑–∞–¥–∞–Ω."
    payload = {
        "model": GROK_MODEL,
        "messages": messages,
        "temperature": 0.2,
    }
    headers = {"Authorization": f"Bearer {GROK_API_KEY}", "Content-Type": "application/json"}
    resp = requests.post(GROK_URL, json=payload, headers=headers, timeout=AI_TIMEOUT)
    logger.info("Grok status=%s body=%s", resp.status_code, _redact(resp.text[:900]))
    resp.raise_for_status()
    data = resp.json()
    choices = data.get("choices", [])
    if not choices:
        return ""
    return ((choices[0].get("message", {}) or {}).get("content") or "").strip()

def ai_chat(messages: List[Dict[str, str]]) -> str:
    engine = (AI_ENGINE or "deepseek").lower()
    if engine in ("deepseek", "deep_seek", "ds"):
        return call_deepseek(messages)
    if engine in ("openai", "gpt", "chatgpt"):
        return call_openai(messages)
    if engine == "gemini":
        return call_gemini(messages)
    if engine in ("grok", "xai"):
        return call_grok(messages)
    return ""

# -------------------------
# JSON extraction + minimal validation
# -------------------------
_JSON_OBJ_RE = re.compile(r"\{.*\}", re.DOTALL)

def extract_json_object(text: str) -> Optional[str]:
    if not text:
        return None
    t = text.strip()
    # Fast path
    if t.startswith("{") and t.endswith("}"):
        return t
    # Try greedy match { ... }
    m = _JSON_OBJ_RE.search(t)
    if m:
        return m.group(0).strip()
    return None

def ensure_2gis_complaint_limit(obj: Dict[str, Any]) -> Dict[str, Any]:
    try:
        platform = (((obj.get("platform_detected") or {}).get("value")) or "unknown").lower()
        complaint = obj.get("complaint") or {}
        text = (complaint.get("text") or "")
        if platform == "2gis" and text:
            if len(text) > 450:
                complaint["text"] = text[:450].rstrip()
            complaint["char_count"] = len(complaint.get("text") or "")
            obj["complaint"] = complaint
    except Exception:
        pass
    return obj

def minimal_shape_fix(obj: Dict[str, Any]) -> Dict[str, Any]:
    # Ensure keys exist with sane defaults (to keep downstream stable)
    def dflt(k, v):
        if k not in obj or obj[k] is None:
            obj[k] = v

    dflt("platform_detected", {"value": "unknown", "confidence": 0.0, "signals": []})
    dflt("review_summary", "")
    dflt("sentiment", {"label": "neutral", "score": 0})
    dflt("emotions", [])
    dflt("aspects", [])
    dflt("facts_vs_opinions", {"facts": [], "opinions": []})
    dflt("pain_points", [])
    dflt("root_cause_hypotheses", [])
    dflt("business_process_flags", [])
    dflt("risks", [])
    dflt("recommendations", [])
    dflt("clarifying_questions", [])
    dflt("policy_check", {"has_possible_violations": False, "possible_violations": [], "notes": ""})
    dflt("public_reply", {"tone": "", "text": ""})
    dflt("complaint", {"needed": False, "reasons": [], "text": "", "char_count": 0})
    return obj

# -------------------------
# Build CX request
# -------------------------
def build_cx_input(
    review_text: str,
    platform: str = "unknown",
    rating: Optional[int] = None,
    review_date: Optional[str] = None,
    meta: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    return {
        "platform": platform or "unknown",
        "rating": rating,
        "review_text": review_text,
        "review_date": review_date,
        "business_context": BUSINESS_CONTEXT,
        "branch/city": BRANCH_CITY,
        "meta": meta or {},
    }

def cx_analyze(input_obj: Dict[str, Any]) -> Tuple[Optional[Dict[str, Any]], str]:
    """
    Returns: (parsed_json_or_none, raw_text)
    """
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT_CX},
        {"role": "user", "content": json.dumps(input_obj, ensure_ascii=False)},
    ]
    raw = ""
    try:
        raw = ai_chat(messages)
    except Exception as e:
        logger.exception("AI transport exception: %s", e)
        return None, raw

    raw = (raw or "").strip()
    json_text = extract_json_object(raw)
    if not json_text:
        return None, raw

    try:
        obj = json.loads(json_text)
        if not isinstance(obj, dict):
            return None, raw
        obj = minimal_shape_fix(obj)
        obj = ensure_2gis_complaint_limit(obj)
        return obj, raw
    except Exception as e:
        logger.warning("JSON parse failed: %s", e)
        return None, raw

# -------------------------
# Telegram formatting + inline keyboard
# -------------------------
def analysis_keyboard(analysis_id: int) -> Dict[str, Any]:
    return {
        "inline_keyboard": [
            [
                {"text": "‚úçÔ∏è –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç", "callback_data": f"reply:{analysis_id}"},
                {"text": "‚ö†Ô∏è –ñ–∞–ª–æ–±–∞", "callback_data": f"complaint:{analysis_id}"},
            ],
            [
                {"text": "üìå –û—Ç–≤–µ—Ç + –∂–∞–ª–æ–±–∞", "callback_data": f"both:{analysis_id}"},
                {"text": "üßæ JSON", "callback_data": f"json:{analysis_id}"},
            ],
        ]
    }

def safe_get(d: Dict[str, Any], path: List[str], default=None):
    cur: Any = d
    for p in path:
        if not isinstance(cur, dict) or p not in cur:
            return default
        cur = cur[p]
    return cur

def format_analysis_summary(obj: Dict[str, Any], analysis_id: int) -> str:
    platform = safe_get(obj, ["platform_detected", "value"], "unknown")
    pconf = safe_get(obj, ["platform_detected", "confidence"], 0.0)
    sentiment = safe_get(obj, ["sentiment", "label"], "neutral")
    sscore = safe_get(obj, ["sentiment", "score"], 0)

    summary = (obj.get("review_summary") or "").strip()
    if len(summary) > 500:
        summary = summary[:500] + "‚Ä¶"

    # Top aspects
    aspects = obj.get("aspects") or []
    top_aspects = []
    if isinstance(aspects, list):
        try:
            aspects_sorted = sorted(
                [a for a in aspects if isinstance(a, dict)],
                key=lambda x: int(x.get("weight") or 0),
                reverse=True,
            )
            for a in aspects_sorted[:3]:
                name = (a.get("name") or "").strip()
                w = a.get("weight")
                if name:
                    top_aspects.append(f"{name}({w})")
        except Exception:
            pass

    complaint_needed = bool(safe_get(obj, ["complaint", "needed"], False))
    policy_bad = bool(safe_get(obj, ["policy_check", "has_possible_violations"], False))

    flags = []
    if complaint_needed:
        flags.append("–∂–∞–ª–æ–±–∞: –¥–∞")
    if policy_bad:
        flags.append("–≤–æ–∑–º–æ–∂–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è: –¥–∞")

    head = [
        f"‚úÖ –ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤. ID: {analysis_id}",
        f"–ü–ª–æ—â–∞–¥–∫–∞: {platform} (conf={pconf:.2f})",
        f"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment} ({sscore})",
    ]
    if top_aspects:
        head.append("–¢–æ–ø-–∞—Å–ø–µ–∫—Ç—ã: " + ", ".join(top_aspects))
    if flags:
        head.append("–§–ª–∞–≥–∏: " + ", ".join(flags))

    return "\n".join(head) + "\n\n" + summary

def format_public_reply(obj: Dict[str, Any]) -> str:
    txt = (safe_get(obj, ["public_reply", "text"], "") or "").strip()
    if not txt:
        return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø—É–±–ª–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç (–ø—É—Å—Ç–æ)."
    return "–ü—É–±–ª–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç:\n\n" + txt

def format_complaint(obj: Dict[str, Any]) -> str:
    needed = bool(safe_get(obj, ["complaint", "needed"], False))
    text = (safe_get(obj, ["complaint", "text"], "") or "").strip()
    char_count = int(safe_get(obj, ["complaint", "char_count"], 0) or 0)
    reasons = safe_get(obj, ["complaint", "reasons"], []) or []
    if not needed:
        return "–ñ–∞–ª–æ–±–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ —Ç–µ–∫—É—â–µ–π –æ—Ü–µ–Ω–∫–µ."
    out = ["–ñ–∞–ª–æ–±–∞ (—á–µ—Ä–Ω–æ–≤–∏–∫):"]
    if reasons and isinstance(reasons, list):
        out.append("–ü—Ä–∏—á–∏–Ω—ã: " + "; ".join([str(x) for x in reasons[:3]]))
    if text:
        out.append("")
        out.append(text)
    if char_count:
        out.append("")
        out.append(f"–î–ª–∏–Ω–∞: {char_count} —Å–∏–º–≤–æ–ª–æ–≤")
    return "\n".join(out).strip()

def format_json_for_chat(obj: Dict[str, Any]) -> str:
    raw = json.dumps(obj, ensure_ascii=False, separators=(",", ":"), indent=2)
    if len(raw) > 3500:
        raw = raw[:3500] + "\n... (–æ–±—Ä–µ–∑–∞–Ω–æ)\n"
    return raw

# -------------------------
# Weekly report
# -------------------------
def build_weekly_report(days: int = 7) -> str:
    since = datetime.utcnow() - timedelta(days=days)
    rows = db_list_analyses_since(since)

    # Normalize rows' result_json for postgres (already dict) / sqlite (dict after parsing)
    results: List[Dict[str, Any]] = []
    for r in rows:
        rj = r.get("result_json")
        if isinstance(rj, dict):
            results.append(rj)

    total = len(results)
    if total == 0:
        return f"–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç (–∑–∞ {days} –¥–Ω.): –∞–Ω–∞–ª–∏–∑–æ–≤ –Ω–µ—Ç."

    # Sentiment distribution
    sent_count: Dict[str, int] = {"positive": 0, "neutral": 0, "mixed": 0, "negative": 0}
    complaint_needed = 0
    policy_flags = 0

    aspect_sum: Dict[str, int] = {}
    pain_sum: Dict[str, int] = {}
    viol_sum: Dict[str, int] = {}

    for obj in results:
        label = (safe_get(obj, ["sentiment", "label"], "neutral") or "neutral").lower()
        if label not in sent_count:
            sent_count[label] = 0
        sent_count[label] += 1

        if bool(safe_get(obj, ["complaint", "needed"], False)):
            complaint_needed += 1
        if bool(safe_get(obj, ["policy_check", "has_possible_violations"], False)):
            policy_flags += 1

        aspects = obj.get("aspects") or []
        if isinstance(aspects, list):
            for a in aspects:
                if not isinstance(a, dict):
                    continue
                name = (a.get("name") or "").strip().lower()
                if not name:
                    continue
                w = int(a.get("weight") or 0)
                aspect_sum[name] = aspect_sum.get(name, 0) + w

        pains = obj.get("pain_points") or []
        if isinstance(pains, list):
            for p in pains:
                if not isinstance(p, dict):
                    continue
                item = (p.get("item") or "").strip().lower()
                if not item:
                    continue
                sev = (p.get("severity") or "low").lower()
                # weighted severity
                score = 1 if sev == "low" else 2 if sev == "medium" else 3
                pain_sum[item] = pain_sum.get(item, 0) + score

        viols = safe_get(obj, ["policy_check", "possible_violations"], []) or []
        if isinstance(viols, list):
            for v in viols:
                if not isinstance(v, dict):
                    continue
                cat = (v.get("category") or "").strip().lower()
                if not cat:
                    continue
                conf = float(v.get("confidence") or 0.0)
                if conf >= 0.6:
                    viol_sum[cat] = viol_sum.get(cat, 0) + 1

    def top_items(d: Dict[str, int], n: int = 5) -> List[str]:
        items = sorted(d.items(), key=lambda x: x[1], reverse=True)[:n]
        return [f"{k} ({v})" for k, v in items]

    lines = []
    lines.append(f"–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç (–∑–∞ {days} –¥–Ω.)")
    lines.append(f"–í—Å–µ–≥–æ –∞–Ω–∞–ª–∏–∑–æ–≤: {total}")
    lines.append("")
    lines.append("–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:")
    lines.append(f"  –ø–æ–∑–∏—Ç–∏–≤: {sent_count.get('positive', 0)}")
    lines.append(f"  –Ω–µ–π—Ç—Ä:   {sent_count.get('neutral', 0)}")
    lines.append(f"  –º–∏–∫—Å:    {sent_count.get('mixed', 0)}")
    lines.append(f"  –Ω–µ–≥–∞—Ç–∏–≤: {sent_count.get('negative', 0)}")
    lines.append("")
    lines.append(f"–ñ–∞–ª–æ–±–∞ –Ω—É–∂–Ω–∞ (complaint.needed=true): {complaint_needed}")
    lines.append(f"–ï—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª: {policy_flags}")
    lines.append("")
    ta = top_items(aspect_sum, 6)
    if ta:
        lines.append("–¢–æ–ø-–∞—Å–ø–µ–∫—Ç—ã (—Å—É–º–º–∞ –≤–µ—Å–æ–≤):")
        for s in ta:
            lines.append("  - " + s)
        lines.append("")
    tp = top_items(pain_sum, 6)
    if tp:
        lines.append("–¢–æ–ø pain-points (–≤–µ—Å –ø–æ severity):")
        for s in tp:
            lines.append("  - " + s)
        lines.append("")
    tv = top_items(viol_sum, 6)
    if tv:
        lines.append("–¢–æ–ø –Ω–∞—Ä—É—à–µ–Ω–∏–π (confidence‚â•0.6):")
        for s in tv:
            lines.append("  - " + s)

    msg = "\n".join(lines).strip()
    if len(msg) > 3800:
        msg = msg[:3800] + "\n... (–æ–±—Ä–µ–∑–∞–Ω–æ)\n"
    return msg

def send_weekly_report(days: int = 7) -> None:
    text = build_weekly_report(days=days)
    # Send to all admins in allowlist mode; if allowlist empty, do nothing to avoid spamming random users
    if not ADMIN_CHAT_IDS:
        logger.warning("Weekly report not sent: ADMIN_CHAT_IDS empty.")
        return
    for cid in ADMIN_CHAT_IDS:
        tg_send_message(cid, text)

# -------------------------
# Background workers
# -------------------------
def background_analyze(chat_id: int, reply_to: int,
                      input_obj: Dict[str, Any], review_id: Optional[int]) -> None:
    try:
        parsed, raw = cx_analyze(input_obj)
        if not parsed:
            # store an error result for traceability
            err_obj = minimal_shape_fix({
                "platform_detected": {"value": "unknown", "confidence": 0.0, "signals": []},
                "review_summary": "",
                "sentiment": {"label": "neutral", "score": 0},
                "policy_check": {"has_possible_violations": False, "possible_violations": [], "notes": "analysis_failed"},
                "public_reply": {"tone": "", "text": ""},
                "complaint": {"needed": False, "reasons": [], "text": "", "char_count": 0},
                "_error": "AI returned invalid JSON",
                "_raw": (raw or "")[:2000],
            })
            analysis_id = db_insert_analysis(review_id, AI_ENGINE, input_obj, err_obj)
            tg_send_message(
                chat_id,
                f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π JSON –æ—Ç –ò–ò. –ê–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Å –æ—à–∏–±–∫–æ–π. ID: {analysis_id}\n"
                f"–ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑ –∏–ª–∏ —Å–º–µ–Ω–∏—Ç—å AI_ENGINE.",
                reply_to=reply_to,
                reply_markup=analysis_keyboard(analysis_id),
            )
            return

        analysis_id = db_insert_analysis(review_id, AI_ENGINE, input_obj, parsed)

        msg = format_analysis_summary(parsed, analysis_id)
        tg_send_message(chat_id, msg, reply_to=reply_to, reply_markup=analysis_keyboard(analysis_id))

    except Exception as e:
        logger.exception("background_analyze failed: %s", e)
        tg_send_message(chat_id, "‚ùå –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ.", reply_to=reply_to)

# -------------------------
# Routes
# -------------------------
@app.get("/")
def health():
    return {
        "ok": True,
        "status": "running",
        "webhook_path": WEBHOOK_PATH,
        "ai_engine": AI_ENGINE,
        "db": "postgres" if USE_POSTGRES else "sqlite",
        "deepseek_url": DEEPSEEK_URL,
        "admin_mode": "allowlist" if ADMIN_CHAT_IDS else "open",
        "has_cron_token": bool(CRON_TOKEN),
    }, 200

@app.get("/cron/weekly")
def cron_weekly():
    token = (request.args.get("token") or "").strip()
    days_s = (request.args.get("days") or "7").strip()
    if not CRON_TOKEN or token != CRON_TOKEN:
        return {"ok": False, "error": "unauthorized"}, 401
    try:
        days = int(days_s)
        days = max(1, min(30, days))
    except Exception:
        days = 7
    send_weekly_report(days=days)
    return {"ok": True, "sent_to": ADMIN_CHAT_IDS, "days": days}, 200

@app.post(WEBHOOK_PATH)
def telegram_webhook():
    update = request.get_json(silent=True) or {}
    logger.info("Update: %s", _redact(json.dumps(update)[:1400]))

    # Handle inline button clicks
    if "callback_query" in update:
        cq = update.get("callback_query") or {}
        cq_id = cq.get("id")
        data = (cq.get("data") or "").strip()
        msg = cq.get("message") or {}
        chat_id = (msg.get("chat") or {}).get("id")

        if cq_id:
            tg_answer_callback_query(cq_id)

        if not chat_id or not data:
            return "ok", 200

        # Expected: action:analysis_id
        try:
            action, sid = data.split(":", 1)
            analysis_id = int(sid)
        except Exception:
            tg_send_message(int(chat_id), "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∫–Ω–æ–ø–∫–∞/–¥–∞–Ω–Ω—ã–µ.", reply_to=None)
            return "ok", 200

        row = db_get_analysis(analysis_id)
        if not row:
            tg_send_message(int(chat_id), f"–ê–Ω–∞–ª–∏–∑ #{analysis_id} –Ω–µ –Ω–∞–π–¥–µ–Ω.", reply_to=None)
            return "ok", 200

        result_obj = row.get("result_json")
        # Postgres returns dict for JSONB; SQLite parsed earlier
        if not isinstance(result_obj, dict):
            try:
                result_obj = json.loads(result_obj or "{}")
            except Exception:
                result_obj = {}

        if action == "reply":
            tg_send_message(int(chat_id), format_public_reply(result_obj))
        elif action == "complaint":
            tg_send_message(int(chat_id), format_complaint(result_obj))
        elif action == "both":
            tg_send_message(int(chat_id), format_public_reply(result_obj))
            tg_send_message(int(chat_id), format_complaint(result_obj))
        elif action == "json":
            tg_send_message(int(chat_id), format_json_for_chat(result_obj))
        else:
            tg_send_message(int(chat_id), "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ.", reply_to=None)

        return "ok", 200

    # Handle normal messages
    message = update.get("message") or update.get("edited_message")
    if not message:
        return "ok", 200

    chat_id = message.get("chat", {}).get("id")
    user_id = message.get("from", {}).get("id")
    msg_id = message.get("message_id")
    text = (message.get("text") or "").strip()

    if not chat_id:
        return "ok", 200

    if not text:
        tg_send_message(int(chat_id), "–Ø –ø—Ä–∏–Ω–∏–º–∞—é —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç üôÇ", reply_to=msg_id)
        return "ok", 200

    logger.info("Parsed: chat_id=%s user_id=%s text=%r", chat_id, user_id, text)

    # Base commands
    if text.startswith("/start"):
        tg_send_message(
            int(chat_id),
            "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–∑—ã–≤–æ–≤.\n\n"
            "–ö–æ–º–∞–Ω–¥—ã:\n"
            "/help ‚Äî –ø–æ–º–æ—â—å\n"
            "/myid ‚Äî –≤–∞—à user_id/chat_id\n"
            "/engine ‚Äî —Ç–µ–∫—É—â–∏–π AI_ENGINE\n"
            "/analyze <—Ç–µ–∫—Å—Ç> ‚Äî –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ (—Å –∫–Ω–æ–ø–∫–∞–º–∏)\n"
            "/analyzereview <id> ‚Äî –∞–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞\n"
            "/weeklyreport [days=7] ‚Äî –æ—Ç—á—ë—Ç (–∞–¥–º–∏–Ω—ã)\n\n"
            "–û—Ç–∑—ã–≤—ã (–∞–¥–º–∏–Ω—ã):\n"
            "/addreview source=yandex|2gis rating=1..5 url=https://... date=YYYY-MM-DD –¢–µ–∫—Å—Ç...\n"
            "/listreviews n=10 [source=yandex|2gis]\n"
            "/review <id>\n"
            "/deletereview <id>\n",
            reply_to=msg_id,
        )
        return "ok", 200

    if text.startswith("/help"):
        tg_send_message(
            int(chat_id),
            "–ê–Ω–∞–ª–∏–∑:\n"
            "/analyze <—Ç–µ–∫—Å—Ç>\n"
            "/analyzereview <id>\n\n"
            "–ü–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –±—É–¥—É—Ç –∫–Ω–æ–ø–∫–∏: –û—Ç–≤–µ—Ç / –ñ–∞–ª–æ–±–∞ / –û–±–∞ / JSON.\n\n"
            "–û—Ç–∑—ã–≤—ã (–∞–¥–º–∏–Ω—ã):\n"
            "/addreview source=yandex rating=5 –û—Ç–ª–∏—á–Ω—ã–π —Å–µ—Ä–≤–∏—Å!\n"
            "/listreviews n=10\n"
            "/review 12\n"
            "/deletereview 12\n\n"
            "–û—Ç—á—ë—Ç (–∞–¥–º–∏–Ω—ã):\n"
            "/weeklyreport days=7\n\n"
            f"AI_ENGINE —Å–µ–π—á–∞—Å: {AI_ENGINE}",
            reply_to=msg_id,
        )
        return "ok", 200

    if text.startswith("/myid"):
        tg_send_message(int(chat_id), f"user_id: {user_id}\nchat_id: {chat_id}", reply_to=msg_id)
        return "ok", 200

    if text.startswith("/engine"):
        tg_send_message(
            int(chat_id),
            f"–¢–µ–∫—É—â–∏–π AI_ENGINE: {AI_ENGINE}\nDeepSeek endpoint: {DEEPSEEK_URL}",
            reply_to=msg_id,
        )
        return "ok", 200

    # Admin gate
    admin_cmds = ("/addreview", "/listreviews", "/review", "/deletereview", "/weeklyreport")
    if any(text.startswith(cmd) for cmd in admin_cmds):
        if not is_admin(int(chat_id)):
            tg_send_message(int(chat_id), "‚õî –ö–æ–º–∞–Ω–¥–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –∞–¥–º–∏–Ω–∞–º.", reply_to=msg_id)
            return "ok", 200

    # Review commands (admin)
    if text.startswith("/addreview"):
        rest = text.replace("/addreview", "", 1).strip()
        reply = message.get("reply_to_message") or {}
        reply_text = (reply.get("text") or "").strip()

        kv, remaining = parse_kv_args(rest)
        review_text = remaining or reply_text
        if not review_text:
            tg_send_message(
                int(chat_id),
                "–ù—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å —Ç–µ–∫—Å—Ç.\n–ü—Ä–∏–º–µ—Ä: /addreview source=yandex rating=5 –û—Ç–ª–∏—á–Ω—ã–π —Å–µ—Ä–≤–∏—Å!",
                reply_to=msg_id,
            )
            return "ok", 200

        source = (kv.get("source") or "manual").lower()
        author = kv.get("author")
        url = kv.get("url")
        published_at = parse_date(kv.get("date") or kv.get("published_at") or "")

        rating: Optional[int] = None
        if "rating" in kv:
            try:
                r = int(kv["rating"])
                if 1 <= r <= 5:
                    rating = r
            except Exception:
                rating = None

        new_id = db_insert_review(
            source=source,
            text=review_text,
            rating=rating,
            author=author,
            url=url,
            published_at=published_at,
            added_by_user_id=int(user_id) if user_id else None,
            added_by_chat_id=int(chat_id) if chat_id else None,
        )
        tg_send_message(int(chat_id), f"‚úÖ –û—Ç–∑—ã–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: #{new_id}\nsource={source}", reply_to=msg_id)
        return "ok", 200

    if text.startswith("/listreviews"):
        rest = text.replace("/listreviews", "", 1).strip()
        kv, _ = parse_kv_args(rest)

        limit = 10
        if "n" in kv:
            try:
                limit = max(1, min(50, int(kv["n"])))
            except Exception:
                limit = 10

        source = (kv.get("source") or "").strip().lower() or None
        rows = db_list_reviews(limit=limit, source=source)
        if not rows:
            tg_send_message(int(chat_id), "–ü–æ–∫–∞ –Ω–µ—Ç –æ—Ç–∑—ã–≤–æ–≤ –≤ –±–∞–∑–µ.", reply_to=msg_id)
            return "ok", 200

        lines = [f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ –æ—Ç–∑—ã–≤—ã (n={len(rows)})" + (f", source={source}" if source else "") + ":"]
        for r in rows:
            lines.append(review_preview(r))
            lines.append("")
        tg_send_message(int(chat_id), "\n".join(lines).strip(), reply_to=msg_id)
        return "ok", 200

    if text.startswith("/review"):
        parts = text.split()
        if len(parts) < 2:
            tg_send_message(int(chat_id), "–ò—Å–ø–æ–ª—å–∑—É–π: /review <id>", reply_to=msg_id)
            return "ok", 200
        try:
            rid = int(parts[1])
        except Exception:
            tg_send_message(int(chat_id), "id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º.", reply_to=msg_id)
            return "ok", 200

        r = db_get_review(rid)
        if not r:
            tg_send_message(int(chat_id), f"–û—Ç–∑—ã–≤ #{rid} –Ω–µ –Ω–∞–π–¥–µ–Ω.", reply_to=msg_id)
            return "ok", 200

        full = [
            f"–û—Ç–∑—ã–≤ #{r.get('id')}",
            f"source: {r.get('source')}",
            f"rating: {r.get('rating')}" if r.get("rating") is not None else "rating: ‚Äî",
            f"author: {r.get('author') or '‚Äî'}",
            f"url: {r.get('url') or '‚Äî'}",
            f"published_at: {r.get('published_at') or '‚Äî'}",
            "",
            (r.get("text") or "").strip(),
        ]
        tg_send_message(int(chat_id), "\n".join(full), reply_to=msg_id)
        return "ok", 200

    if text.startswith("/deletereview"):
        parts = text.split()
        if len(parts) < 2:
            tg_send_message(int(chat_id), "–ò—Å–ø–æ–ª—å–∑—É–π: /deletereview <id>", reply_to=msg_id)
            return "ok", 200
        try:
            rid = int(parts[1])
        except Exception:
            tg_send_message(int(chat_id), "id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º.", reply_to=msg_id)
            return "ok", 200

        ok = db_delete_review(rid)
        tg_send_message(int(chat_id), "‚úÖ –£–¥–∞–ª–µ–Ω–æ." if ok else "–ù–µ –Ω–∞–π–¥–µ–Ω–æ.", reply_to=msg_id)
        return "ok", 200

    # Weekly report (admin)
    if text.startswith("/weeklyreport"):
        rest = text.replace("/weeklyreport", "", 1).strip()
        kv, _ = parse_kv_args(rest)
        days = 7
        if "days" in kv:
            try:
                days = max(1, min(30, int(kv["days"])))
            except Exception:
                days = 7
        tg_send_message(int(chat_id), build_weekly_report(days=days), reply_to=msg_id)
        return "ok", 200

    # Analysis commands (anyone)
    if text.startswith("/analyze"):
        analyze_text = text.replace("/analyze", "", 1).strip()
        if not analyze_text:
            tg_send_message(int(chat_id), "–ò—Å–ø–æ–ª—å–∑—É–π: /analyze <—Ç–µ–∫—Å—Ç>", reply_to=msg_id)
            return "ok", 200

        tg_send_message(int(chat_id), "–ü—Ä–∏–Ω—è–ª ‚úÖ –ì–æ—Ç–æ–≤–ª—é –∞–Ω–∞–ª–∏–∑‚Ä¶", reply_to=msg_id)

        input_obj = build_cx_input(
            review_text=analyze_text,
            platform="unknown",
            rating=None,
            review_date=None,
            meta={"via": "command_analyze"},
        )
        threading.Thread(
            target=background_analyze,
            args=(int(chat_id), int(msg_id), input_obj, None),
            daemon=True,
        ).start()
        return "ok", 200

    if text.startswith("/analyzereview"):
        parts = text.split()
        if len(parts) < 2:
            tg_send_message(int(chat_id), "–ò—Å–ø–æ–ª—å–∑—É–π: /analyzereview <id>", reply_to=msg_id)
            return "ok", 200
        try:
            rid = int(parts[1])
        except Exception:
            tg_send_message(int(chat_id), "id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º.", reply_to=msg_id)
            return "ok", 200

        r = db_get_review(rid)
        if not r:
            tg_send_message(int(chat_id), f"–û—Ç–∑—ã–≤ #{rid} –Ω–µ –Ω–∞–π–¥–µ–Ω.", reply_to=msg_id)
            return "ok", 200

        # map source -> platform if possible
        source = (r.get("source") or "unknown").lower()
        platform = "unknown"
        if "2gis" in source or "2–≥–∏—Å" in source:
            platform = "2gis"
        elif "yandex" in source or "—è–Ω–¥" in source:
            platform = "yandex"

        rating = r.get("rating")
        try:
            rating = int(rating) if rating is not None else None
        except Exception:
            rating = None

        published_at = r.get("published_at")
        review_date = None
        if published_at:
            review_date = str(published_at)

        meta = {
            "via": "saved_review",
            "author": r.get("author"),
            "url": r.get("url"),
            "review_id": rid,
            "source": source,
        }

        tg_send_message(int(chat_id), f"–ü—Ä–∏–Ω—è–ª ‚úÖ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –æ—Ç–∑—ã–≤ #{rid}‚Ä¶", reply_to=msg_id)

        input_obj = build_cx_input(
            review_text=(r.get("text") or "").strip(),
            platform=platform,
            rating=rating,
            review_date=review_date,
            meta=meta,
        )
        threading.Thread(
            target=background_analyze,
            args=(int(chat_id), int(msg_id), input_obj, rid),
            daemon=True,
        ).start()
        return "ok", 200

    # default
    tg_send_message(
        int(chat_id),
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "/analyze <—Ç–µ–∫—Å—Ç> ‚Äî –∞–Ω–∞–ª–∏–∑ (—Å –∫–Ω–æ–ø–∫–∞–º–∏)\n"
        "/analyzereview <id> ‚Äî –∞–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞\n"
        "/help ‚Äî –ø–æ–º–æ—â—å",
        reply_to=msg_id,
    )
    return "ok", 200
